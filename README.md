# Neural Networks & Deep Learning - Vorlesungsfolien

[![LaTeX](https://img.shields.io/badge/LaTeX-beamer-blue.svg)](https://github.com/FNeubuerger/DL_Slides)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“š Ãœber diese Vorlesung

Diese Vorlesung bietet eine umfassende EinfÃ¼hrung in **Neural Networks** und **Deep Learning**. Die Folien kombinieren theoretische Grundlagen mit praktischen Anwendungen und enthalten ausfÃ¼hrliche mathematische Herleitungen.

## ğŸ¯ Lernziele

Nach dieser Vorlesung kÃ¶nnen Studierende:
- Mathematische Grundlagen neuronaler Netze verstehen und anwenden
- Verschiedene Netzarchitekturen (MLP, CNN, RNN, LSTM, VAE, GAN) erklÃ¤ren
- Backpropagation-Algorithmus mathematisch herleiten
- Praktische Entscheidungen zwischen Deep Learning und klassischem ML treffen

## ğŸ“– Inhalt der Vorlesung

### 1. Mathematische Grundlagen (Folien 1-50)
- **Lineare Algebra Refresher**: Vektoren, Matrizen, Eigenwerte
- **Analysis**: Gradienten, partielle Ableitungen, Kettenregel
- **Wahrscheinlichkeitstheorie**: Verteilungen, Bayes'sche Statistik
- **Optimierung**: Gradientenabstieg, KonvexitÃ¤t
- **Informationstheorie**: Entropie, KL-Divergenz
- **Statistik**: Hypothesentests, Maximum Likelihood

### 2. Grundlagen Neuronaler Netze (Folien 51-100)
- **Perceptron**: Lineare Klassifikation, mathematische Herleitung
- **Multi-Layer Perceptron (MLP)**: Universal Approximation Theorem
- **Aktivierungsfunktionen**: Sigmoid, ReLU, Softmax
- **Backpropagation**: Mathematische Herleitung und Implementierung
- **Verlustfunktionen**: Cross-Entropy, Mean Squared Error

### 3. Deep Learning Architekturen (Folien 101-200)
- **Convolutional Neural Networks (CNNs)**:
  - Convolution Operation und mathematische Grundlagen
  - Pooling, Feature Maps, Translation Equivarianz
  - Anwendungen in Computer Vision
- **Recurrent Neural Networks (RNNs)**:
  - Sequentielle Datenverarbeitung
  - Vanishing Gradient Problem
- **Long Short-Term Memory (LSTM)**:
  - Gate-Mechanismen, Cell State
  - Mathematische Herleitung der Forget-, Input- und Output-Gates

### 4. Generative Modelle (Folien 201-250)
- **Variational Autoencoders (VAEs)**:
  - Evidence Lower Bound (ELBO)
  - Reparameterization Trick
  - Latente ReprÃ¤sentationen
- **Generative Adversarial Networks (GANs)**:
  - Adversarial Training, Minimax-Spiel
  - Lipschitz-Constraint und WGAN
  - Mode Collapse und LÃ¶sungsansÃ¤tze

### 5. Praktische Anwendungen (Folien 251-300)
- **Wann Deep Learning vs. klassisches ML?**: Praktische Entscheidungshilfen
- **Projektarbeit**: Hands-on Erfahrung mit realen DatensÃ¤tzen

## ğŸ› ï¸ Technische Details

- **LaTeX Engine**: LuaLaTeX (empfohlen fÃ¼r Unicode-UnterstÃ¼tzung)
- **Document Class**: Beamer fÃ¼r PrÃ¤sentationen
- **Sprache**: Deutsch (mit Polyglossia)
- **Mathematik**: umfangreiche Nutzung von amsmath, mathtools
- **Bibliographie**: BibTeX mit DOI-Links fÃ¼r alle Referenzen

## ğŸš€ Kompilierung

### Voraussetzungen
- TeX Live 2024 oder spÃ¤ter
- LuaLaTeX Engine
- Folgende LaTeX-Pakete: `beamer`, `polyglossia`, `amsmath`, `mathtools`, `csquotes`, `smartdiagram`

### Kompilieren
```bash
# Mit latexmk (empfohlen)
latexmk -lualatex -synctex=1 -interaction=nonstopmode main.tex

# Manuell
lualatex main.tex
bibtex main
lualatex main.tex
lualatex main.tex
```

## ğŸ“ Projektstruktur

```
DL_Slides/
â”œâ”€â”€ main.tex                 # Hauptdokument
â”œâ”€â”€ lit.bib                  # Bibliographie mit DOIs
â”œâ”€â”€ beamerthemefhswf.sty     # Hochschul-Theme
â”œâ”€â”€ images/                  # Alle Abbildungen und Diagramme
â”‚   â”œâ”€â”€ CNN.png
â”‚   â”œâ”€â”€ LSTM_*.png
â”‚   â”œâ”€â”€ GAN.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logos/                   # Hochschul-Logos
â””â”€â”€ README.md               # Diese Datei
```

## ğŸ“ FÃ¼r Studierende

### Empfohlene Vorbereitung
- Grundlagen in Python und NumPy
- Lineare Algebra und Analysis (Refresher in der Vorlesung enthalten)
- Interesse an mathematischen Herleitungen

## ğŸ“š WeiterfÃ¼hrende Ressourcen

- **BÃ¼cher**:
  - Goodfellow, I., Bengio, Y., Courville, A.: "Deep Learning" (MIT Press)
  - Bishop, C.: "Pattern Recognition and Machine Learning"
- **Online-Kurse**:
  - CS231n: Convolutional Neural Networks (Stanford)
  - CS224n: Natural Language Processing (Stanford)
- **Praktische Tools**:
  - PyTorch, TensorFlow
  - Jupyter Notebooks
  - Google Colab fÃ¼r GPU-Zugang

## ğŸ¤ Contributing

VerbesserungsvorschlÃ¤ge und Fehlerkorrekturen sind willkommen:
1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/improvement`)
3. Committe deine Ã„nderungen (`git commit -am 'Add improvement'`)
4. Push zum Branch (`git push origin feature/improvement`)
5. Erstelle einen Pull Request

## ğŸ“„ Lizenz

Dieses Projekt steht unter der MIT-Lizenz - siehe [LICENSE](LICENSE) fÃ¼r Details.

## ğŸ‘¨â€ğŸ« Autor

**Felix NeubÃ¼rger**  
Fachhochschule SÃ¼dwestfalen  
Fachbereich Informatik und Naturwissenschaften

---

*Diese Vorlesung wurde entwickelt, um Studierenden eine solide theoretische Grundlage und praktische Erfahrung im Bereich Deep Learning zu vermitteln.*