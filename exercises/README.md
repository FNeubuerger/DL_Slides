# Deep Learning Ãœbungen - FH SÃ¼dwestfalen

## Ãœbersicht

Diese Sammlung enthÃ¤lt 5 umfassende ÃœbungsblÃ¤tter fÃ¼r die Deep Learning Vorlesung. Jede Ãœbung baut auf den vorherigen auf und deckt verschiedene Aspekte des Deep Learning ab.

## Ãœbungsstruktur

### ğŸ“š Ãœbung 1: Mathematische Grundlagen und EinfÃ¼hrung
- **Themen**: Lineare Algebra, Aktivierungsfunktionen, Perceptron, XOR-Problem
- **Schwerpunkt**: Mathematische Fundamente, erste Implementierungen
- **Bearbeitungszeit**: 2 Wochen
- **Punkte**: 85 + 10 Bonus

**Lernziele:**
- Verstehen der mathematischen Grundlagen
- Implementierung von Aktivierungsfunktionen
- LÃ¶sung des XOR-Problems mit MLPs
- Grundlagen der Gradientenberechnung

### ğŸ§  Ãœbung 2: Multi-Layer Perceptrons und Backpropagation
- **Themen**: Backpropagation-Algorithmus, MLP-Implementierung, Vanishing Gradients
- **Schwerpunkt**: Tieferes VerstÃ¤ndnis des Lernprozesses
- **Bearbeitungszeit**: 3 Wochen
- **Punkte**: 85 + 15 Bonus

**Lernziele:**
- Mathematische Herleitung von Backpropagation
- VollstÃ¤ndige MLP-Implementierung von Grund auf
- Experimentelle Evaluierung verschiedener Architekturen
- Verstehen von Overfitting und Regularisierung

### ğŸ–¼ï¸ Ãœbung 3: Convolutional Neural Networks (CNNs)
- **Themen**: 2D-Convolution, Pooling, CNN-Architekturen, Computer Vision
- **Schwerpunkt**: Bildverarbeitung und rÃ¤umliche Strukturen
- **Bearbeitungszeit**: 3 Wochen
- **Punkte**: 105 + 15 Bonus

**Lernziele:**
- Mathematik der Convolution-Operation
- CNN-Implementierung fÃ¼r Bildklassifikation
- Verstehen von Feature Maps und hierarchischem Lernen
- Praktische Anwendung auf MNIST

### ğŸ”„ Ãœbung 4: Recurrent Neural Networks und Sequence Modeling
- **Themen**: RNNs, LSTMs, BPTT, Attention, Sequence-to-Sequence
- **Schwerpunkt**: Zeitreihendaten und sequentielle Modellierung
- **Bearbeitungszeit**: 3 Wochen
- **Punkte**: 105 + 15 Bonus

**Lernziele:**
- Backpropagation Through Time (BPTT)
- LSTM-Architektur und Gating-Mechanismen
- Anwendungen auf Zeitreihen und Textgeneration
- Attention-Mechanismen

### ğŸ¨ Ãœbung 5: Generative Modelle und Advanced Deep Learning
- **Themen**: Autoencoders, VAEs, GANs, Transfer Learning
- **Schwerpunkt**: Generative Modellierung und praktische Anwendungen
- **Bearbeitungszeit**: 4 Wochen
- **Punkte**: 105 + 45 Bonus/Projekt

**Lernziele:**
- Variational Autoencoders und probabilistische Modellierung
- Generative Adversarial Networks und Spieltheorie
- Praktische Deep Learning Techniken
- End-to-End Projektarbeit

## ğŸ“‹ Voraussetzungen

### Mathematische Grundlagen
- **Lineare Algebra**: Matrixoperationen, Eigenwerte, Normen
- **Analysis**: Partielle Ableitungen, Kettenregel, Optimierung
- **Statistik**: Wahrscheinlichkeitstheorie, Verteilungen
- **Numerik**: Numerische StabilitÃ¤t, Finite Differenzen

### Programmierkenntnisse
- **Python**: Grundlagen, OOP, NumPy, Matplotlib
- **Datenstrukturen**: Arrays, Listen, Dictionaries
- **Algorithmen**: Gradient Descent, Backpropagation
- **Optional**: PyTorch/TensorFlow fÃ¼r erweiterte Aufgaben

## ğŸ› ï¸ Technische Umgebung

### Erforderliche Bibliotheken
```python
# Kern-Bibliotheken
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats

# Datenverarbeitung
from sklearn.datasets import fetch_openml, make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Erweiterte Visualisierung
import seaborn as sns

# Optional fÃ¼r Bonusaufgaben
import torch
import tensorflow as tf
```

### Installation
```bash
pip install numpy matplotlib scipy scikit-learn seaborn
# Optional:
pip install torch torchvision tensorflow
```

## ğŸ“Š Bewertungssystem

### Punkteverteilung
| Ãœbung | Grundpunkte | Bonuspunkte | Gesamt |
|-------|-------------|-------------|---------|
| 1 | 85 | 10 | 95 |
| 2 | 85 | 15 | 100 |
| 3 | 105 | 15 | 120 |
| 4 | 105 | 15 | 120 |
| 5 | 105 | 45 | 150 |
| **Gesamt** | **485** | **100** | **585** |

### NotenschlÃ¼ssel
- **1,0-1,3 (Sehr gut)**: 90-100% der Grundpunkte
- **1,7-2,3 (Gut)**: 75-89% der Grundpunkte
- **2,7-3,3 (Befriedigend)**: 60-74% der Grundpunkte
- **3,7-4,0 (Ausreichend)**: 50-59% der Grundpunkte

### Bonuspunkte
- Bonuspunkte kÃ¶nnen Grundpunkte kompensieren
- Maximale Gesamtnote: 1,0
- Besonders kreative LÃ¶sungen werden extra gewÃ¼rdigt

## ğŸ“ Abgaberichtlinien

### Format
- **Code**: Jupyter Notebooks (.ipynb) mit vollstÃ¤ndiger AusfÃ¼hrung
- **Mathematik**: PDF-Dokument mit LaTeX-formatierten Herleitungen
- **Diskussion**: Interpretation der Ergebnisse und Beobachtungen
- **Plots**: Alle Visualisierungen mit Beschriftung

### Struktur
```
abgabe_uebungX_nachname/
â”œâ”€â”€ uebungX_nachname.ipynb    # Hauptnotebook
â”œâ”€â”€ mathematik_uebungX.pdf    # Mathematische Herleitungen
â”œâ”€â”€ ergebnisse/               # Generierte Plots und Modelle
â”‚   â”œâ”€â”€ plots/
â”‚   â””â”€â”€ models/
â””â”€â”€ README.md                 # Kurze Zusammenfassung
```

### QualitÃ¤tskriterien
- **Reproduzierbarkeit**: Code lÃ¤uft ohne Fehler durch
- **Dokumentation**: AusfÃ¼hrliche Kommentare und Markdown
- **Mathematik**: VollstÃ¤ndige, korrekte Herleitungen
- **Analyse**: Kritische Diskussion der Ergebnisse

## ğŸ¯ Lernstrategie

### Empfohlenes Vorgehen
1. **Theorie zuerst**: Verstehen Sie die mathematischen Grundlagen
2. **Kleine Schritte**: Implementieren Sie zunÃ¤chst einfache Versionen
3. **Testen**: Verwenden Sie kleine, bekannte DatensÃ¤tze zum Debugging
4. **Visualisieren**: Plotten Sie Zwischenergebnisse fÃ¼r besseres VerstÃ¤ndnis
5. **Experimentieren**: Variieren Sie Hyperparameter systematisch

### HÃ¤ufige Probleme und LÃ¶sungen
- **Numerische InstabilitÃ¤t**: Verwenden Sie numerisch stabile Implementierungen
- **Overfitting**: Implementieren Sie Regularisierung und Cross-Validation
- **Debugging**: ÃœberprÃ¼fen Sie Dimensionen und Gradienten numerisch
- **Performance**: Optimieren Sie erst nach korrekter Implementierung

## ğŸ¤ Zusammenarbeit

### Erlaubt
- Diskussion von Konzepten und AnsÃ¤tzen
- Gemeinsames Debugging von Implementierungsproblemen
- Austausch von Referenzen und Ressourcen
- 2er-Teams fÃ¼r Brainstorming (individuelle Abgabe!)

### Nicht erlaubt
- Kopieren von Code oder mathematischen Herleitungen
- Austausch von kompletten LÃ¶sungen
- Verwendung von ChatGPT/KI fÃ¼r direkte LÃ¶sungsgenerierung
- Plagiate jeder Art

## ğŸ“š ZusÃ¤tzliche Ressourcen

### Empfohlene Literatur
- **Deep Learning** - Ian Goodfellow, Yoshua Bengio, Aaron Courville
- **Pattern Recognition and Machine Learning** - Christopher Bishop
- **Neural Networks for Pattern Recognition** - Christopher Bishop

### Online-Ressourcen
- [Deep Learning Specialization - Coursera](https://www.coursera.org/specializations/deep-learning)
- [CS231n - Stanford](http://cs231n.stanford.edu/)
- [Distill.pub](https://distill.pub/) - Interaktive ML-Artikel
- [Papers with Code](https://paperswithcode.com/) - SOTA-Implementierungen

### Implementierungsbeispiele
- [Andrej Karpathy's Blog](http://karpathy.github.io/)
- [Christopher Olah's Blog](http://colah.github.io/)
- [Sebastian Ruder's Blog](https://ruder.io/)

## ğŸ†˜ Hilfe und Support

### Sprechstunden
- **Wann**: Dienstags 14:00-16:00
- **Wo**: BÃ¼ro XYZ oder online
- **Anmeldung**: Per E-Mail im Voraus

### E-Mail Support
- **Antwortzeit**: Innerhalb von 24-48 Stunden
- **Format**: Spezifische Fragen mit Code-Snippets
- **Betreff**: "DL Ãœbung X - Kurze Beschreibung"

### Peer Learning
- **Discord/Slack**: Gemeinsamer Kanal fÃ¼r Diskussionen
- **Study Groups**: Selbstorganisierte Lerngruppen
- **Office Hours**: Offene Fragestunden vor Abgabeterminen

## ğŸ† Erfolg messen

### Nach jeder Ãœbung sollten Sie kÃ¶nnen:
- **Ãœbung 1**: Grundlegende neuronale Netze von Grund auf implementieren
- **Ãœbung 2**: Komplexe Architekturen entwerfen und trainieren
- **Ãœbung 3**: Computer Vision Probleme mit CNNs lÃ¶sen
- **Ãœbung 4**: Sequentielle Daten mit RNNs/LSTMs modellieren
- **Ãœbung 5**: Generative Modelle verstehen und anwenden

### Portfolio-Entwicklung
Am Ende des Kurses haben Sie:
- Ein vollstÃ¤ndiges Deep Learning Framework implementiert
- Mehrere praktische Anwendungen entwickelt
- Tiefes VerstÃ¤ndnis der mathematischen Grundlagen
- FÃ¤higkeit zur kritischen Analyse von ML-Modellen
- Basis fÃ¼r fortgeschrittene Deep Learning Forschung

---

**Viel Erfolg bei den Ãœbungen! ğŸš€**

Bei Fragen stehen wir gerne zur VerfÃ¼gung.