\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}

\geometry{margin=2.5cm}
\pagestyle{fancy}
\fancyhf{}
\rhead{Deep Learning - Übung 2}
\lhead{FH Südwestfalen}
\rfoot{Seite \thepage}

% Python code style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    breaklines=true,
    showstringspaces=false
}

\title{\textbf{Deep Learning - Übungsblatt 2} \\ \large Multi-Layer Perceptrons und Backpropagation}
\author{Fachhochschule Südwestfalen}
\date{\today}

\begin{document}

\maketitle

\section*{Voraussetzungen}
\begin{itemize}
    \item Übungsblatt 1 sollte erfolgreich bearbeitet worden sein
    \item Grundkenntnisse in Python und NumPy
    \item Verständnis von Matrixoperationen und partiellen Ableitungen
\end{itemize}

\section{Backpropagation-Algorithmus }

\subsection{Mathematische Herleitung }

\textbf{Aufgabe 1.1:} Gegeben sei ein 3-Schicht-MLP für binäre Klassifikation:

\textbf{Architektur:}
\begin{align}
\text{Input Layer:} &\quad \mathbf{x} \in \mathbb{R}^2 \\
\text{Hidden Layer 1:} &\quad \mathbf{z}^{(1)} = \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}, \quad \mathbf{a}^{(1)} = \sigma(\mathbf{z}^{(1)}) \in \mathbb{R}^3 \\
\text{Hidden Layer 2:} &\quad \mathbf{z}^{(2)} = \mathbf{W}^{(2)} \mathbf{a}^{(1)} + \mathbf{b}^{(2)}, \quad \mathbf{a}^{(2)} = \sigma(\mathbf{z}^{(2)}) \in \mathbb{R}^2 \\
\text{Output Layer:} &\quad z^{(3)} = \mathbf{w}^{(3)T} \mathbf{a}^{(2)} + b^{(3)}, \quad \hat{y} = \sigma(z^{(3)}) \in \mathbb{R}
\end{align}

\textbf{Verlustfunktion:} $L = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$ (Binary Cross-Entropy)

\begin{enumerate}[(a)]
    \item Berechnen Sie $\frac{\partial L}{\partial z^{(3)}}$ (zeigen Sie, dass $\frac{\partial L}{\partial z^{(3)}} = \hat{y} - y$) 
    
    \item Berechnen Sie die Gradienten für die Ausgabeschicht:
    \begin{itemize}
        \item $\frac{\partial L}{\partial \mathbf{w}^{(3)}}$ 
        \item $\frac{\partial L}{\partial b^{(3)}}$ 
    \end{itemize}
    
    \item Berechnen Sie $\frac{\partial L}{\partial \mathbf{a}^{(2)}}$ mittels Kettenregel 
    
    \item Berechnen Sie die Gradienten für Hidden Layer 2:
    \begin{itemize}
        \item $\frac{\partial L}{\partial \mathbf{z}^{(2)}}$ (verwenden Sie $\frac{\partial \sigma}{\partial z} = \sigma(z)(1-\sigma(z))$) 
        \item $\frac{\partial L}{\partial \mathbf{W}^{(2)}}$ und $\frac{\partial L}{\partial \mathbf{b}^{(2)}}$ 
    \end{itemize}
    
    \item Berechnen Sie analog die Gradienten für Hidden Layer 1: $\frac{\partial L}{\partial \mathbf{W}^{(1)}}$ und $\frac{\partial L}{\partial \mathbf{b}^{(1)}}$ 
\end{enumerate}

\subsection{Numerisches Beispiel }

\textbf{Aufgabe 1.2:} Führen Sie einen kompletten Forward- und Backward-Pass durch:

\textbf{Gegeben:}
\begin{align}
\mathbf{x} &= \begin{pmatrix} 0.5 \\ 0.8 \end{pmatrix}, \quad y = 1 \\
\mathbf{W}^{(1)} &= \begin{pmatrix} 0.2 & 0.1 \\ -0.3 & 0.4 \\ 0.6 & -0.2 \end{pmatrix}, \quad \mathbf{b}^{(1)} = \begin{pmatrix} 0.1 \\ -0.2 \\ 0.3 \end{pmatrix} \\
\mathbf{W}^{(2)} &= \begin{pmatrix} 0.4 & -0.1 & 0.3 \\ 0.2 & 0.5 & -0.4 \end{pmatrix}, \quad \mathbf{b}^{(2)} = \begin{pmatrix} 0.1 \\ -0.1 \end{pmatrix} \\
\mathbf{w}^{(3)} &= \begin{pmatrix} 0.6 \\ -0.3 \end{pmatrix}, \quad b^{(3)} = 0.2
\end{align}

\begin{enumerate}[(a)]
    \item Berechnen Sie den Forward Pass: $\mathbf{z}^{(1)}, \mathbf{a}^{(1)}, \mathbf{z}^{(2)}, \mathbf{a}^{(2)}, z^{(3)}, \hat{y}$ 
    \item Berechnen Sie den Loss $L$ 
    \item Berechnen Sie alle Gradienten des Backward Pass 
\end{enumerate}

\section{Implementierung eines MLP }

\subsection{MLP-Klasse }

\textbf{Aufgabe 2.1:} Implementieren Sie eine vollständige MLP-Klasse:

\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt

class MLP:
    def __init__(self, layer_sizes, activation='sigmoid', learning_rate=0.01):
        """
        layer_sizes: Liste mit Anzahl Neuronen pro Schicht [input, hidden1, hidden2, ..., output]
        activation: 'sigmoid', 'relu', oder 'tanh'
        learning_rate: Lernrate
        """
        # Ihre Implementierung hier
        pass
    
    def _initialize_weights(self):
        """Xavier/He-Initialisierung der Gewichte"""
        # Ihre Implementierung hier
        pass
    
    def _forward(self, X):
        """Forward Pass - speichert Zwischenergebnisse fuer Backprop"""
        # Ihre Implementierung hier
        pass
    
    def _backward(self, X, y):
        """Backward Pass - berechnet Gradienten"""
        # Ihre Implementierung hier
        pass
    
    def train(self, X, y, epochs=1000, batch_size=None, verbose=False):
        """Training mit Mini-Batch Gradient Descent"""
        # Ihre Implementierung hier
        pass
    
    def predict(self, X):
        """Vorhersagen fuer neue Daten"""
        # Ihre Implementierung hier
        pass
    
    def score(self, X, y):
        """Accuracy fuer Klassifikation"""
        # Ihre Implementierung hier
        pass
\end{lstlisting}

\textbf{Bewertungskriterien:}
\begin{itemize}
    \item Korrekte Gewichtsinitialisierung 
    \item Forward Pass Implementation 
    \item Backward Pass Implementation 
    \item Training Loop mit Mini-Batches 
    \item Predict und Score Methoden 
    \item Code-Qualität und Dokumentation 
\end{itemize}

\subsection{Experimentelle Evaluierung }

\textbf{Aufgabe 2.2:} Testen Sie Ihr MLP auf verschiedenen Datensätzen:

\begin{enumerate}[(a)]
    \item \textbf{XOR-Problem:} Trainieren Sie ein MLP mit [2, 4, 1] Architektur 
    \begin{itemize}
        \item Plotten Sie den Loss über die Epochen
        \item Visualisieren Sie die Entscheidungsgrenze
        \item Erreichen Sie 100\% Accuracy
    \end{itemize}
    
    \item \textbf{Spiralen-Datensatz:} Erstellen Sie einen 2D-Spiralen-Datensatz und klassifizieren Sie ihn 
    \begin{lstlisting}
def make_spirals(n_samples=200, noise=0.1):
    """Erstellt 2D-Spiralen-Datensatz"""
    t = np.linspace(0, 4*np.pi, n_samples//2)
    x1 = t * np.cos(t) + noise * np.random.randn(n_samples//2)
    y1 = t * np.sin(t) + noise * np.random.randn(n_samples//2)
    x2 = -t * np.cos(t) + noise * np.random.randn(n_samples//2)
    y2 = -t * np.sin(t) + noise * np.random.randn(n_samples//2)
    
    X = np.vstack([np.column_stack([x1, y1]), np.column_stack([x2, y2])])
    y = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)])
    return X, y
    \end{lstlisting}
    
    \item \textbf{Hyperparameter-Tuning:} Experimentieren Sie mit verschiedenen Architekturen und Hyperparametern 
    \begin{itemize}
        \item Anzahl versteckter Schichten: [2, 8], [2, 16, 8], [2, 32, 16, 8]
        \item Lernraten: 0.001, 0.01, 0.1
        \item Aktivierungsfunktionen: sigmoid, relu, tanh
        \item Dokumentieren Sie die besten Ergebnisse
    \end{itemize}
\end{enumerate}

\section{Vertiefende Fragen }

\textbf{Aufgabe 3.1:} Theoretische Analyse 

\begin{enumerate}[(a)]
    \item \textbf{Vanishing Gradient Problem:} 
    \begin{itemize}
        \item Erklären Sie, warum tiefe Netzwerke mit Sigmoid-Aktivierung Probleme beim Training haben 
        \item Berechnen Sie die maximale Ableitung der Sigmoid-Funktion 
        \item Wie löst ReLU dieses Problem? 
    \end{itemize}
    
    \item \textbf{Initialisierung:} Erklären Sie Xavier- und He-Initialisierung mathematisch. Warum ist zufällige Initialisierung wichtig? 
\end{enumerate}

\textbf{Aufgabe 3.2:} Praktische Probleme 

\begin{enumerate}[(a)]
    \item \textbf{Overfitting:} 
    \begin{itemize}
        \item Erstellen Sie einen kleinen Datensatz (50 Samples) und trainieren Sie ein überparametrisiertes Netzwerk 
        \item Implementieren Sie Early Stopping 
        \item Vergleichen Sie Training- und Validation-Loss 
    \end{itemize}
    
    \item \textbf{Learning Rate:} Experimentieren Sie mit verschiedenen Lernraten und dokumentieren Sie den Einfluss auf das Training 
\end{enumerate}

\section{Bonusaufgaben (15 )}

\textbf{Aufgabe 4.1:} Erweiterte Implementierungen 

\begin{enumerate}[(a)]
    \item \textbf{Momentum:} Erweitern Sie Ihr MLP um Momentum-basierte Optimierung 
    \begin{equation}
    v_t = \beta v_{t-1} + (1-\beta) \nabla_\theta L(\theta_{t-1})
    \end{equation}
    \begin{equation}
    \theta_t = \theta_{t-1} - \alpha v_t
    \end{equation}
    
    \item \textbf{Regularisierung:} Implementieren Sie L2-Regularisierung 
    \begin{equation}
    L_{reg} = L + \lambda \sum_{l} ||\mathbf{W}^{(l)}||_2^2
    \end{equation}
    
    \item \textbf{Adaptive Learning Rates:} Implementieren Sie RMSprop oder Adam 
\end{enumerate}

\section*{Abgabehinweise}
\begin{itemize}
    \item Abgabe als Jupyter Notebook (.ipynb) oder Python-Skript mit separatem PDF-Report
    \item Alle Plots und numerischen Ergebnisse dokumentieren
    \item Code muss reproduzierbar sein (feste Random Seeds)
    \item Mathematische Herleitungen vollständig ausschreiben
    \item Diskussion der Ergebnisse und Beobachtungen
\end{itemize}

\section*{Bewertungsschema}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Aufgabenbereich} & \textbf{Punkte} \\
\hline
Backpropagation-Algorithmus & 30 \\
MLP-Implementierung & 35 \\
Vertiefende Fragen & 20 \\
\hline
\textbf{Gesamt} & \textbf{85} \\
\hline
Bonusaufgaben & +15 \\
\hline
\end{tabular}
\end{center}

\section*{Tipps für die Implementierung}
\begin{itemize}
    \item Verwenden Sie \texttt{np.random.seed()} für reproduzierbare Ergebnisse
    \item Implementieren Sie numerische Gradientenprüfung zur Verifikation
    \item Starten Sie mit einfachen Problemen (XOR) vor komplexeren Datensätzen
    \item Visualisieren Sie Zwischenergebnisse für Debugging
    \item Nutzen Sie Vektorisierung für Effizienz
\end{itemize}

\end{document}

